# run_local_export_jsonl.py
# -*- coding: utf-8 -*-
import json
from pathlib import Path
from datetime import datetime, timedelta

from crawler_core import crawl_many

# ====== í¬ë¡¤ë§ ëŒ€ìƒ ì¢…ëª© ======
STOCKS = [
    {"name": "ì‚¼ì„±ì „ì", "code": "005930"},
    {"name": "NAVER",   "code": "035420"},
    {"name": "ì¹´ì¹´ì˜¤",   "code": "035720"},
    {"name": "í˜„ëŒ€ì°¨",   "code": "005380"},
]

# ====== ì¶œë ¥ ë£¨íŠ¸ ======
OUT_ROOT = Path("./news_raw")

# ====== ìˆ˜ì§‘í•  ë‚ ì§œ êµ¬ê°„(í¬í•¨ ë²”ìœ„) ======
# ìš”êµ¬: 7.7~7.11, 7.14~7.18, 7.21~7.25, 7.28~7.31, 8.4
# ì—°ë„ëŠ” ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•˜ê²Œ 2025ë…„ìœ¼ë¡œ ê°€ì •
DATE_BLOCKS = [
    ("2025.08.06", "2025.08.8"),
    ("2025.08.11", "2025.08.14"),
]
SINGLE_DATES = ["2025.08.04"]  # ë‹¨ì¼ ë‚ ì§œ

# ====== ì˜µì…˜ ======
MAX_PAGES_PER_STOCK = 200  # ë„¤ì´ë²„ ì¢…ëª©ë‰´ìŠ¤ í˜ì´ì§€ íƒìƒ‰ ìµœëŒ€ í˜ì´ì§€
PROGRESS_EVERY = 10        # ì§„í–‰ìƒí™© í‘œì‹œ ì£¼ê¸°(ê±´)

def safe_filename(name: str) -> str:
    """íŒŒì¼/í´ë”ëª… ì•ˆì „í™”: ê¸ˆì§€ë¬¸ì ì œê±°, ê³µë°± ì••ì¶•"""
    import re
    s = str(name).strip()
    s = re.sub(r"[\\/:*?\"<>|]+", "_", s)  # ê¸ˆì§€ë¬¸ì â†’ _
    s = re.sub(r"\s+", "_", s)             # ë‹¤ì¤‘ ê³µë°± â†’ _
    return s

def date_token(date_str: str) -> str:
    """'YYYY.MM.DD' â†’ 'YYYYMMDD'"""
    return date_str.replace(".", "")

def _daterange_inclusive(start_str: str, end_str: str):
    """'YYYY.MM.DD' ~ 'YYYY.MM.DD' í¬í•¨ ë²”ìœ„ë¡œ í•˜ë£¨ì”© ìƒì„±"""
    start = datetime.strptime(start_str, "%Y.%m.%d")
    end   = datetime.strptime(end_str,   "%Y.%m.%d")
    cur = start
    while cur <= end:
        yield cur.strftime("%Y.%m.%d")
        cur += timedelta(days=1)

def _all_target_dates():
    dates = []
    for s, e in DATE_BLOCKS:
        dates.extend(list(_daterange_inclusive(s, e)))
    dates.extend(SINGLE_DATES)
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    dates = sorted(set(dates))
    return dates

def crawl_one_day(day_str: str):
    """í•˜ë£¨ì¹˜(YYYY.MM.DD) ìˆ˜ì§‘ â†’ news_raw/YYYYMMDD/{ì¢…ëª©}.jsonl"""
    dt_tok = date_token(day_str)
    day_dir = OUT_ROOT / dt_tok
    day_dir.mkdir(parents=True, exist_ok=True)

    for stock in STOCKS:
        stock_name = stock["name"]
        out_path = day_dir / f"{safe_filename(stock_name)}.jsonl"

        print(f"\nğŸ“Œ [{stock_name}] {day_str} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        count = 0
        with open(out_path, "w", encoding="utf-8") as f:
            for r in crawl_many([stock], day_str, max_pages=MAX_PAGES_PER_STOCK):
                f.write(json.dumps(r, ensure_ascii=False) + "\n")
                count += 1
                if count % PROGRESS_EVERY == 0:
                    print(f"  â”œâ”€ ì§„í–‰: {count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"âœ… [{stock_name}] ì™„ë£Œ: {count}ê±´ ì €ì¥ â†’ {out_path}")

def main():
    targets = _all_target_dates()
    print("=== ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ ===")
    for d in targets:
        print(" -", d)
    print("=====================\n")

    for day in targets:
        crawl_one_day(day)

if __name__ == "__main__":
    main()
